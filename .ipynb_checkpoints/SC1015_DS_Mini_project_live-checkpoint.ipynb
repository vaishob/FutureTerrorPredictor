{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CQ6E4hvNSFov"
   },
   "source": [
    "# SC1015 Data Science Project - Ryan Yu & Vaishob \n",
    "---\n",
    "\n",
    "# Introduction to topic\n",
    "\n",
    "Terrorism is defined as political violence in an asymmetrical conflict that is designed to induce terror and psychic fear (sometimes indiscriminate) through the violent victimization and destruction of noncombatant targets (sometimes iconic symbols).\n",
    "\n",
    "The Global Terrorism Database (GTD)™ is the most comprehensive unclassified database of terrorist attacks in the world. It is an open-source database, which provides information on domestic and international terrorist attacks around the world since 1970, and now includes more than 200,000 events. For each event, a wide range of information is available, including the date and location of the incident, the weapons used, nature of the target, the number of casualties, and – when identifiable – the group or individual responsible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mlTAQZ0UpNXg"
   },
   "source": [
    "## Practical Motivation\n",
    "\n",
    " ---\n",
    "In the past, many terror attacks have been tipped off by intelligence agencies to the government. But the government, for varying reasons, have disregarded such calls (resulting in loss of many lives in some cases). By building a model that can accurately predict the success of future terror attacks, we can help respective governments to delegate resources more wisely and, more importantly, save more lives.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e9E60fwDpTMH"
   },
   "source": [
    "## Data Preparation & Cleaning\n",
    "\n",
    " ---\n",
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "pt5dAJMySb5M"
   },
   "outputs": [],
   "source": [
    "# Basic Libraries\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sb\n",
    "import matplotlib.pyplot as plt # we only need pyplot\n",
    "import matplotlib.patches as mpatches \n",
    "import matplotlib.font_manager as font_manager\n",
    "sb.set() # set the default Seaborn style for graphics\n",
    "\n",
    "# Import essential models and functions from sklearn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn import tree\n",
    "import plotly.express as px\n",
    "import folium\n",
    "from folium.plugins import HeatMapWithTime\n",
    "\n",
    "from DataSynthesizer.DataDescriber import DataDescriber\n",
    "from DataSynthesizer.DataGenerator import DataGenerator\n",
    "from DataSynthesizer.ModelInspector import ModelInspector\n",
    "from DataSynthesizer.lib.utils import read_json_file, display_bayesian_network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 921
    },
    "id": "qWg7FN9kSt2z",
    "outputId": "b809657c-5089-4d6d-d2d8-141b7746a83e",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# read file using .read_excel\n",
    "df = pd.read_excel(\"globalterrorismdb_0221dist.xlsx\")\n",
    "df "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K0iGyXnGPu2c"
   },
   "source": [
    "### Exploring dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G8_d4FrgpsGE"
   },
   "outputs": [],
   "source": [
    "#Features that we require for prediction, this will be used for our RandomForestClassifier model\n",
    "feature= [      'iyear', #Iyear is the year the event occurred.\n",
    "                'extended', #extended means is the event lasted for more than 24 hours\n",
    "                'vicinity', #is the attack in the vicinity of city?\n",
    "                'doubtterr',#any doubts that it is a terrorist aattack?\n",
    "                'multiple', # is the event linked to other attacks?\n",
    "                'suicide', # is this a suicide attack?\n",
    "                'claimed', #has any terrorist group claimed the attack\n",
    "                'property',#any property damage?\n",
    "                'ishostkid',#any hostages or kidnappings?\n",
    "                'country',#country\n",
    "                'region',#region \n",
    "                'attacktype1',# method of attack used\n",
    "                'targtype1',#type of target\n",
    "                'weaptype1',#weapon used\n",
    "                'longitude',\n",
    "                 'latitude'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Label encoding to map each categorical data to a numeric value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data cleaning\n",
    "cntry = dict(zip(df.country,df.country_txt))\n",
    "weapx = dict(zip(df.weaptype1,df.weaptype1_txt))\n",
    "atk = dict(zip(df.attacktype1,df.attacktype1_txt))\n",
    "targ = dict(zip(df.targtype1,df.targtype1_txt))\n",
    "print(\"Country \\n\",cntry)\n",
    "print(\"\\n Weapon type \\n\",weapx)\n",
    "print(\"\\n Attack type \\n\",atk)\n",
    "print(\"\\n Target type \\n\",targ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Isolate and clean the required data by dropping all null values to obtain a dataframe, `frame`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat= [      'iyear', #Iyear is the year the event occurred.\n",
    "                'imonth',\n",
    "                'extended', #extended means is the event lasted for more than 24 hours\n",
    "                'vicinity', #is the attack in the vicinity of city?\n",
    "                'doubtterr',#any doubts that it is a terrorist aattack?\n",
    "                'multiple', # is the event linked to other attacks?\n",
    "                'suicide', # is this a suicide attack?\n",
    "                'claimed', #has any terrorist group claimed the attack\n",
    "                'property',#any property damage?\n",
    "                'ishostkid',#any hostages or kidnappings?\n",
    "                'country',#country\n",
    "                'region',#region \n",
    "                'attacktype1',# method of attack used\n",
    "                'targtype1',#type of target\n",
    "                'weaptype1',#weapon used\n",
    "                'longitude',\n",
    "                 'latitude',\n",
    "       'success'\n",
    "]\n",
    "frame=df[feat]\n",
    "#dropping all columns with NaN values\n",
    "frame = frame.dropna()\n",
    "frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RdG_8oH7pwLR"
   },
   "source": [
    "## Exploratory Data Analysis & Data Visualisation\n",
    "\n",
    " ---\n",
    "\n",
    "##### Initially we use the original dataframe, `df`. Subsequently we will use the `frame` dataframe that was created"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Uni-variate Visualization**\n",
    "\n",
    "Identify patterns occuring in the dataset using univariate visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What proportion of past terrorist attacks was successful?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy = df.copy()\n",
    "df_copy[\"success\"] = df[\"success\"].replace({1:\"Successful Operation\", 0:\"Failed Operation\"})\n",
    "df_copy[\"success\"].value_counts().plot(kind=\"pie\", figsize=(20,10), autopct=\"%1.1f%%\", cmap=\"summer\")\n",
    "plt.title(\"Successful & Failed Operations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X2cDbo5AhG5z"
   },
   "source": [
    "**What is the trend of number of terrorist attacks over the years?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qAEQy53bp1Ff",
    "outputId": "29a53e37-ef51-4801-9cad-13297e5015c6",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "barplot = pd.value_counts(df['iyear'])\\\n",
    ".sort_index()\\\n",
    ".plot\\\n",
    ".bar(width=0.8, figsize=(16, 8), align='center', title=\"Yearly count of terrorist attacks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is the trend of terrorist activities by region?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terror_region = df['region_txt']\n",
    "terror_region.rename(\"Region\",\n",
    "          inplace=True)\n",
    "terror_region=pd.crosstab(df['iyear'],terror_region)\n",
    "terror_region.plot(color=sb.color_palette('Set1',12))\n",
    "fig=plt.gcf()\n",
    "fig.set_size_inches(18,6)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Here we note a distinct spike in the number of attacks in 2012-2014 period. This is reflected across all major regions, namely Middle East, North Africa, Sub-Saharan Africa, South Asia, Australasia & Oceania and Eastern Europe.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Which Terrorist Group has carried out the largest number of attacks?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_name=df_copy[\"gname\"]\n",
    "group_name.dropna()\n",
    "# Terrorist groups in the Dataset\n",
    "print(\"Number of Terrorist groups : \\t\\t\\t\\t\\t\\t\\t    \", len(group_name.unique()))\n",
    "\n",
    "# Number of attacks per terrorist groups \n",
    "print(group_name.value_counts())\n",
    "#sb.catplot(y = \"gname\", data = df_copy['gname'], kind = \"count\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While there is a vast number of terrorist groups out there, it is interesting to note the proportion of attacks without a designated perpetrator (89231 in total). This shows how sparse information is in an area that concerns national security"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Drop all Unknown entries (Invalid Terrorist Group name)\n",
    "gname = group_name.copy()\n",
    "gname.replace('Unknown', np.nan, inplace=True)\n",
    "gname.dropna()\n",
    "\n",
    "print(gname.value_counts())\n",
    "\n",
    "#df['gname'].value_counts()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Distribution of the top 5 terrorist groups in terms of quanitity of attacks**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = {\n",
    "    'Group Name': ['Taliban', 'Islamic State of Iraq and the Levant (ISIL)', 'Shining Path (SL)', 'Al-Shabaab', 'FMLN'],\n",
    "    'Values': [10094, 6864, 4563, 4126, 3351]\n",
    "}\n",
    "\n",
    "group_prop = pd.DataFrame(data)\n",
    "group_prop.reset_index(drop=True, inplace=True)\n",
    "\n",
    "Groups = group_prop['Group Name']\n",
    "Values = group_prop['Values']\n",
    "\n",
    "# compute the proportion of each group with respect to the total\n",
    "total_values = 10094 + 6864 + 4563 + 4126 + 3351\n",
    "category_proportions = [(float(Value) / total_values) for Value in Values]\n",
    "\n",
    "print(category_proportions)\n",
    "\n",
    "width = 50    # width of chart \n",
    "height = 10    # height of chart\n",
    "\n",
    "total_num_tiles = width * height # total number of tiles\n",
    "\n",
    "tiles_per_category = [round(proportion * total_num_tiles) for proportion in category_proportions]\n",
    "\n",
    "# initialize the waffle chart as an empty matrix\n",
    "waffle_chart = np.zeros((height, width))\n",
    "\n",
    "# define indices to loop through waffle chart\n",
    "category_index = 0\n",
    "tile_index = 0\n",
    "\n",
    "# populate the waffle chart\n",
    "for col in range(width):\n",
    "    for row in range(height):\n",
    "        tile_index +=1\n",
    "        \n",
    "        # if the number of tiles populated for the current category is equal to its allocated tiles...\n",
    "        if tile_index > sum(tiles_per_category[0:category_index]):\n",
    "            # ... proceed to the next category\n",
    "            category_index += 1\n",
    "        \n",
    "        # set the class value to an integer, which increases with class\n",
    "        waffle_chart[row, col] = category_index\n",
    "\n",
    "# instantiate figure object\n",
    "fig = plt.figure()\n",
    "\n",
    "# use matshow to display the waffle chart\n",
    "colormap = plt.cm.viridis\n",
    "plt.matshow(waffle_chart, cmap=colormap)\n",
    "#plt.colorbar()\n",
    "\n",
    "# get the axis \n",
    "ax = plt.gca()\n",
    "\n",
    "# set minor ticks\n",
    "ax.set_xticks(np.arange(-.5, (width), 1), minor=True)\n",
    "ax.set_yticks(np.arange(-.5, (height), 1), minor=True)\n",
    "\n",
    "# add gridlines based on minor ticks\n",
    "ax.grid(which='minor', color='w', linestyle='-', linewidth=2)\n",
    "ax.set_title('Proportion of attacks by top 5 terrorist groups', fontdict={'fontsize': 22, 'fontweight': 'medium'})\n",
    "\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "\n",
    "# compute cumulative sum of individual categories to match color schemes between char and legend\n",
    "values_cumsum = np.cumsum(data['Values'])\n",
    "total_values = values_cumsum[len(values_cumsum) - 1]\n",
    "\n",
    "# create legend \n",
    "legend_handles = []\n",
    "#for i, category in enumerate(data['Group Name']):\n",
    " #   label_str = category + ' (' + str(data['Values'][i]) + ') '\n",
    "  #  color_val = colormap(int(i+1))/len(Groups)\n",
    "   # legend_handles.append(mpatches.Patch(color=color_val, label=label_str))\n",
    "\n",
    "for indx, company_name in enumerate(Groups):\n",
    "    label_text = '{1}: ({0})'.format(Values[indx], company_name)\n",
    "    color_value = colormap(int(indx+1)/len(data['Group Name']))\n",
    "    legend_handles.append(mpatches.Patch(color=color_value, label=label_text))\n",
    "\n",
    "font = font_manager.FontProperties(\n",
    "    family='Arial',\n",
    "    weight='bold',\n",
    "    style='normal',\n",
    "    size=14,\n",
    ")\n",
    "\n",
    "# add legend to chart\n",
    "plt.legend(handles=legend_handles,\n",
    "          loc='lower center', \n",
    "          ncol=len(Groups),\n",
    "          prop=font, \n",
    "          bbox_to_anchor=(0., -0.3, 0.95, .1)\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop cells with predicted_terrorism 0\n",
    "#clean=df.drop(newdf[newdf.Predicted_Terrorism ==0].index)\n",
    "a=df[\"country_txt\"].value_counts()\n",
    "adf= pd.DataFrame(a)\n",
    "#Reset so the index will now be a column\n",
    "adf.reset_index(inplace=True)\n",
    "#Now rename the columns\n",
    "adf=adf.rename(columns={'index':'country','country_txt':'count'})\n",
    "print(adf.head(n=10).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GV0QO6LAhVsT"
   },
   "source": [
    "**Where do these attacks occur?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(16,8))\n",
    "sb.countplot('country_txt',data=df,palette='inferno',order=df[\"country_txt\"].value_counts().index[:10])\n",
    "plt.xticks(rotation=90)\n",
    "plt.title('Countries Most Affected By Terrorism')\n",
    "plt.xlabel(\"Country\")\n",
    "plt.grid(axis=\"y\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### It appears that Middle East, South Asia are the biggest hotspots for terror attacks. They are followed by South American nations, Philippines as well as United Kingdom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Which Types of Attacks are Common?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "attack_df = df['attacktype1_txt']\n",
    "attack_df.rename(\"Attack Type\",\n",
    "          inplace=True)\n",
    "attack_df.value_counts().plot(kind='pie',figsize=[16,12],autopct='%1.1f%%')\n",
    "plt.title(\"Most Common Attack Methods\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Bombing/Explosion comes out on top, comprising almost half of all terrorist attacks. It is followed by Armed Assault, with 23.6% of attacks. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Popular targets of terrorists?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targ_df = df['targtype1_txt']\n",
    "targ_df.rename(\"Target Type\",\n",
    "          inplace=True)\n",
    "plt.subplots(figsize=(15,6))\n",
    "sb.countplot(targ_df,palette='inferno',order=targ_df.value_counts().index)\n",
    "plt.xticks(rotation=90)\n",
    "plt.title('Popular Targets')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Private citizens and property seem to be most commonly targeted by terrorist groups. This is followed by the Military, Police, Government and Businesses\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Common Types of Attacks by Region?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(df.region_txt,df.attacktype1_txt).plot.barh(stacked=True,width=1,color=sb.color_palette('RdBu',9))\n",
    "fig=plt.gcf()\n",
    "fig.set_size_inches(16,8)\n",
    "plt.ylabel(\"Region\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In most countries, we see that **bombing/explosion** is the most commonly-used attacking means. It is worth noting that for regions of **Sub-Saharan Africa**, as well as the **Central Americas & Caribbean**, **armed assault** is more prevalent than bombing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analytic Visualization\n",
    "---\n",
    "### In order to find patterns, we need to find better visualization techniques...\n",
    "\n",
    "##### Is there a way to incorporate past-year attacks on the world map? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coords = frame[['latitude','longitude','success']]\n",
    "coords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, OrderedDict\n",
    "data=defaultdict(list)\n",
    "for x in frame.itertuples():\n",
    "    if (x.success): #add only if it is successful\n",
    "        data[x.iyear].append([x.latitude,x.longitude])\n",
    "\n",
    "data = OrderedDict(sorted(data.items(), key=lambda t: t[0]))\n",
    "print(data)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting up the world countries data URL\n",
    "url = 'https://raw.githubusercontent.com/python-visualization/folium/master/examples/data'\n",
    "country_shapes = f'{url}/world-countries.json'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Interactive Folium heatmap to visualise distribution of attacks over the years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_map = folium.Map(location=[0,0], zoom_start=2,tiles=\"cartodbpositron\",max_bounds=True)\n",
    "hm = HeatMapWithTime(data=list(data.values()),\n",
    "                     index=list(data.keys()), \n",
    "                     radius=12,\n",
    "                     auto_play=True,\n",
    "                     max_opacity=0.3)\n",
    "hm.add_to(init_map)\n",
    "init_map.save(\"index.html\")\n",
    "\n",
    "init_map\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  Time: There is a stark contrast in distribution of terrorist attacks between **pre-1998** and **post-1998**. In the latter period the number of attacks has increased with the distribution not changing significantly. \n",
    "  \n",
    "  Question: Would it be advisable to use the data from pre-1998 even though the distribution is clearly significantly different from current and possibly future attacks? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem: \n",
    "## *To predict the success of terrorist attacks in 2021 with the highest possible accuracy using Machine Learning models.*\n",
    "\n",
    "Can the success of an attack be predicted from GTD-exclusive features?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nVRpnI9rp99_"
   },
   "source": [
    "## Machine Learning\n",
    "\n",
    " ---\n",
    " ### Here we use RandomForestClassifier as our prediction model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cd3ZyIwGqAr9",
    "outputId": "d4ad0533-0a6a-42ca-cef4-64d928caaec4"
   },
   "outputs": [],
   "source": [
    "X = df[feature].fillna(0) # Assign chosen features to X.\n",
    "y = df.success\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "\n",
    "\n",
    "model = RandomForestClassifier(n_estimators=10)\n",
    "\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KTgTFNJ8u_6Z",
    "outputId": "b850791f-3d45-4d37-cf34-4514d3f5f7e2",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy score of prediction model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0SJ84a8QqEfT",
    "outputId": "2b81bee0-1cbe-420b-a40c-31e420ec8ec3"
   },
   "outputs": [],
   "source": [
    "#Accuracy score\n",
    "\n",
    "y_pred_test = model.predict(X_test)\n",
    "\n",
    "accuracy_score(y_test,y_pred_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confusion Matrix of Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "br5Kg6btvIwW",
    "outputId": "b98a0a58-c86c-4e0b-9cee-89e346d13ccc"
   },
   "outputs": [],
   "source": [
    "#View CONFUSION matrix\n",
    "#In percentages and in numbers.\n",
    "\n",
    "c = confusion_matrix(y_test,y_pred_test)\n",
    "f, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "sb.heatmap(c/np.sum(c),  annot = True, annot_kws={\"size\": 18} ,fmt='.2%',ax=axes[0])\n",
    "sb.heatmap(c,  annot = True, fmt=\".0f\", annot_kws={\"size\": 18}, ax = axes[1])\n",
    "\n",
    "print(classification_report(y_test, y_pred_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing Between Random Forest And Decision Tree "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recall the Legendary-Total Dataset\n",
    "X = df[feature].fillna(0) # Assign chosen features to X.\n",
    "y = df.success\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "\n",
    "# Decision Tree using Train Data\n",
    "dectree = DecisionTreeClassifier(max_depth = 2)  # create the decision tree object\n",
    "dectree.fit(X_train, y_train)                    # train the decision tree model\n",
    "\n",
    "\n",
    "y_train_pred = dectree.predict(X_train)\n",
    "y_test_pred = dectree.predict(X_test)\n",
    "\n",
    "print(\"Goodness of Fit of Model \\tTrain Dataset\")\n",
    "print(\"Classification Accuracy \\t:\", dectree.score(X_train, y_train))\n",
    "print()\n",
    "\n",
    "# Check the Goodness of Fit (on Test Data)\n",
    "print(\"Goodness of Fit of Model \\tTest Dataset\")\n",
    "print(\"Classification Accuracy \\t:\", dectree.score(X_test, y_test))\n",
    "print()\n",
    "# Plot the Confusion Matrix for Train and Test\n",
    "f, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "sb.heatmap(confusion_matrix(y_train, y_train_pred),\n",
    "           annot = True, fmt=\".0f\", annot_kws={\"size\": 18}, ax = axes[0])\n",
    "sb.heatmap(confusion_matrix(y_test, y_test_pred), \n",
    "           annot = True, fmt=\".0f\", annot_kws={\"size\": 18}, ax = axes[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fine Tuning Using Gini Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hdmtfjdWvMss",
    "outputId": "bbfd3d56-de1a-4cf2-8095-466aed57f514",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Fine tuning, Checking the IMPORTANCE OF EACH feature\n",
    "important_features = model.feature_importances_\n",
    "forest_importances = pd.Series(important_features, index=feature)\n",
    "fig, ax = plt.subplots(figsize=(16,10))\n",
    "forest_importances.plot.bar(yerr=important_features, ax=ax)\n",
    "ax.set_title(\"Feature importances\")\n",
    "ax.set_ylabel(\"Gini Importance\")\n",
    "#The higher the value, the more important the feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here we attempt to further optimise accuracy of our model by omitting less-important features (with respect to Gini importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "57gJftpNvUQv",
    "outputId": "0d7dc683-e617-450a-9107-cf0ec1a05c5f"
   },
   "outputs": [],
   "source": [
    "#This shows the importancce of each features in predicting Success.\n",
    "#We scrap\n",
    "feature2= ['iyear',\n",
    "                'property',\n",
    "                'country',\n",
    "                'attacktype1',\n",
    "                'targtype1',\n",
    "                'weaptype1',\n",
    "]\n",
    "\n",
    "\n",
    "X = df[feature2].fillna(0) # Assign chosen features to X.\n",
    "y = df.success\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=0)\n",
    "\n",
    "\n",
    " #Use random forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "model = RandomForestClassifier(n_estimators=10)\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "#Accuracy score\n",
    "\n",
    "y_pred_test = model.predict(X_test)\n",
    "\n",
    "print(accuracy_score(y_test,y_pred_test))\n",
    "\n",
    "  \n",
    "#View CONFUSION matrix\n",
    "#In percentages and in numbers.\n",
    "\n",
    "c = confusion_matrix(y_test,y_pred_test)\n",
    "f, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "sb.heatmap(c/np.sum(c),  annot = True, annot_kws={\"size\": 18} ,fmt='.2%',ax=axes[0])\n",
    "sb.heatmap(c,  annot = True, fmt=\".0f\", annot_kws={\"size\": 18}, ax = axes[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dfb-c4CNvbPH",
    "outputId": "59f48f89-a754-4259-a8b3-35bc4ed29338"
   },
   "source": [
    "### Using our classification model to predict the 2021 terrorism events\n",
    "How accurately can our model predict them?\n",
    "#### Here we collect sample data of 2021 terrorist attacks from Wikipedia.org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BIhnUp5GvtlP",
    "outputId": "a86976e7-cd9b-46e4-af62-88fb6d192dc1"
   },
   "outputs": [],
   "source": [
    "#LIST OF 2021 ATTACKS, # CAN WE PREDICT THEM?\n",
    "html_data = pd.read_html('https://en.wikipedia.org/wiki/List_of_terrorist_incidents_in_2021')\n",
    "html_data\n",
    "print(len(html_data)) #b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ohe-eQ3CvwR8",
    "outputId": "f0c3f40f-5808-4355-a11c-6905faafc755",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "html_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iYRH_I7VrwI2",
    "outputId": "11a8b930-ce6c-4b95-aca0-3ac1f5a0f202"
   },
   "outputs": [],
   "source": [
    "data2021 = pd.read_excel(\"2021terrorattacks.xlsx\")\n",
    "data2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FaXrqGMXsI6C",
    "outputId": "33917f23-9ec7-48a4-eda9-020b5eac395a",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predicted = model.predict(data2021)\n",
    "\n",
    "results= pd.DataFrame(predicted,columns = [\"Predicted_Terrorism\"])\n",
    "\n",
    "summary = pd.concat([data2021,results],axis=1)\n",
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we witness a **100% hit rate**, as our model managed to accurately predict **all 18 terrorism acts in 2021** (as per on Wikipedia) as successful "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using our classification model to predict success of future terrorist attacks in 2022\n",
    "In order to predict future events, we need to generate a set of synthetic data first. Here we use `Linear Regression` and `Bayesian Network` to do the respective predictions\n",
    "#### We use the year to predict terrorism count. We create a new dataframe, `lin`, that stores the year and corresponding terrorism count. Then we use `year` to predict `count`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Linear regression Model:\n",
    "#Use year to, predict Terrorism count.\n",
    "#Create new DF, Year count and terrorism\n",
    "from sklearn.linear_model import LinearRegression\n",
    "da = pd.value_counts(df['iyear'])\n",
    "lin = pd.DataFrame(da)\n",
    "#Reset so the index will now be a column\n",
    "lin.reset_index(inplace=True)\n",
    "lin=lin.rename(columns={'index':'year','iyear':'count'})\n",
    "lin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Linear Regression Model using the Train set\n",
    "#### Use `lin_year` as *Predictor* and `lin_count` as *Response*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the Dataset into Train and Test\n",
    "lin_year = pd.DataFrame(lin['year'])\n",
    "lin_count = pd.DataFrame(lin['count'])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(lin_year,lin_count, test_size = 0.25)\n",
    "\n",
    "# Linear Regression using Train Data\n",
    "linreg = LinearRegression()         # create the linear regression object\n",
    "linreg.fit(X_train, y_train)        # train the linear regression model\n",
    "\n",
    "#Using Our linear regression model to predict 2022.\n",
    "d = {'year':[2022]}\n",
    "predict= pd.DataFrame(d);\n",
    "predicted_count = linreg.predict(predict)\n",
    "pc=predicted_count[0][0]\n",
    "pc = int(pc)\n",
    "print(pc)\n",
    "#Predicted count = 9768\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Coefficients of the Linear Regression line\n",
    "print('Intercept \\t: b = ', linreg.intercept_)\n",
    "print('Coefficients \\t: a = ', linreg.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the regression line by prediction using the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Formula for the Regression line\n",
    "regline_x = X_train\n",
    "regline_y = linreg.intercept_ + linreg.coef_ * X_train\n",
    "\n",
    "# Plot the Linear Regression line\n",
    "# Predict Total values corresponding to HP Train\n",
    "y_train_pred = linreg.predict(X_train)\n",
    "\n",
    "# Plot the Linear Regression line based on the model\n",
    "f = plt.figure(figsize=(16, 8))\n",
    "plt.scatter(X_train, y_train)\n",
    "plt.scatter(X_train, y_train_pred, color = \"r\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Goodness of Fit of the Model on TRAIN Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#goodness of fit of model ON TRAIN SET.\n",
    "\n",
    "# Explained Variance (R^2)\n",
    "print(\"Explained Variance (R^2) \\t:\", linreg.score(X_train, y_train))\n",
    "\n",
    "# Mean Squared Error (MSE)\n",
    "def mean_sq_err(actual, predicted):\n",
    "    '''Returns the Mean Squared Error of actual and predicted values'''\n",
    "    return np.mean(np.square(np.array(actual) - np.array(predicted)))\n",
    "\n",
    "\n",
    "mse = mean_sq_err(X_train, y_train_pred)\n",
    "print(\"Mean Squared Error (MSE) \\t:\", mse)\n",
    "print(\"Root Mean Squared Error (RMSE) \\t:\", np.sqrt(mse))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test the Linear Regression model `linreg` using the Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Goodness of fit of model on TEST Set.\n",
    "# Predict Total count corresponding to year_Test\n",
    "y_test_pred = linreg.predict(X_test)\n",
    "\n",
    "# Plot the Predictions\n",
    "f = plt.figure(figsize=(16, 8))\n",
    "plt.scatter(X_test, y_test, color = \"green\")\n",
    "plt.scatter(X_test, y_test_pred, color = \"red\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check how good the predictions are on the Test Set    \n",
    "#### **Metrics :** Explained Variance and Mean Squared Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explained Variance (R^2) ON TEST DATA\n",
    "print(\"Explained Variance (R^2) \\t:\", linreg.score(X_test, y_test))\n",
    "\n",
    "# Mean Squared Error (MSE)\n",
    "def mean_sq_err(actual, predicted):\n",
    "    '''Returns the Mean Squared Error of actual and predicted values'''\n",
    "    return np.mean(np.square(np.array(actual) - np.array(predicted)))\n",
    "\n",
    "mse = mean_sq_err(y_test, y_test_pred)\n",
    "print(\"Mean Squared Error (MSE) \\t:\", mse)\n",
    "print(\"Root Mean Squared Error (RMSE) \\t:\", np.sqrt(mse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We use Linear Regression model to predict count of terrorist attacks in 2022, and store it as `pc`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using Our linear regression model to predict 2022.\n",
    "d = {'year':[2022]}\n",
    "predict= pd.DataFrame(d);\n",
    "predicted_count = linreg.predict(predict)\n",
    "pc=predicted_count[0][0]\n",
    "pc = int(pc)\n",
    "print(pc)\n",
    "#Predicted count = 9768"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rHBe5i-C22qt"
   },
   "outputs": [],
   "source": [
    "# input dataset\n",
    "input_data = 'terrorismdb.csv'\n",
    "# location of two output files\n",
    "mode = 'correlated_attribute_mode'\n",
    "description_file = f'description.json'\n",
    "synthetic_data = f'sythetic_data.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here we use Bayesian Network to generate a synthetic dataset representing 2022's data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GDenaKOF24Jl"
   },
   "outputs": [],
   "source": [
    "# An attribute is categorical if its domain size is less than this threshold.\n",
    "# Here modify the threshold to adapt to the domain size of \"education\" (which is 14 in input dataset).\n",
    "threshold_value = 20\n",
    "\n",
    "# specify categorical attributes\n",
    "categorical_attributes = {'country': True, 'attacktype1':True,'targtype1':True,'weaptype1':True,'property':True}\n",
    "\n",
    "# A parameter in Differential Privacy. It roughly means that removing a row in the input dataset will not \n",
    "# change the probability of getting the same output more than a multiplicative difference of exp(epsilon).\n",
    "# Increase epsilon value to reduce the injected noises. Set epsilon=0 to turn off differential privacy.\n",
    "epsilon = 1\n",
    "\n",
    "# The maximum number of parents in Bayesian network, i.e., the maximum number of incoming edges.\n",
    "degree_of_bayesian_network = 2\n",
    "\n",
    "# Number of tuples generated in synthetic dataset.\n",
    "num_tuples_to_generate = pc #We use OUR PREDICTED VALUE OF 2022, FROM OUR LINREG MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "39g50eOc25-s"
   },
   "outputs": [],
   "source": [
    "describer = DataDescriber(category_threshold=threshold_value)\n",
    "describer.describe_dataset_in_correlated_attribute_mode(dataset_file=input_data, \n",
    "                                                        epsilon=epsilon, \n",
    "                                                        k=degree_of_bayesian_network,\n",
    "                                                        attribute_to_is_categorical=categorical_attributes)\n",
    "describer.save_dataset_description_to_file(description_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SpwtLt3e26gs"
   },
   "outputs": [],
   "source": [
    "generator = DataGenerator()\n",
    "generator.generate_dataset_in_correlated_attribute_mode(num_tuples_to_generate, description_file)\n",
    "generator.save_synthetic_data(synthetic_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### After saving the synthetically-generated dataset, we then read both datasets to verify that they are similar "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "36HVHEkq28EN"
   },
   "outputs": [],
   "source": [
    "# Read both datasets using Pandas.\n",
    "input_df = pd.read_csv(input_data, skipinitialspace=True)\n",
    "synthetic_df = pd.read_csv(synthetic_data)\n",
    "# Read attribute description from the dataset description file.\n",
    "attribute_description = read_json_file(description_file)['attribute_description']\n",
    "\n",
    "inspector = ModelInspector(input_df, synthetic_df, attribute_description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparing Histograms (Left-hand side is Real data, Right-hand side is Synthetic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "81hAgoz228u8",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for attribute in synthetic_df.columns:\n",
    "    inspector.compare_histograms(attribute)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we note that for each column field, the general distribution of values across the bins is similar between the **real data** and the **synthesized data**. This shows that our synthetic dataset is similar in nature to past real-life data and we can go ahead and use it to predict using our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparing Heatmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v4jogahU2-Bd",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "inspector.mutual_information_heatmap()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we note that the respective correlations among the six variables are generally similar across both the real data (**Private**) and the newly-predicted data (**Synthetic**)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We read our synthesised data into a dataframe, `syndata`, to carry out our final prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v_vK6IS32_c9"
   },
   "outputs": [],
   "source": [
    "syndata = pd.read_csv('sythetic_data.csv')\n",
    "syndata['iyear']=2022 #set all year to 2022\n",
    "syndata.property=np.where(syndata['property']<0,0,1)\n",
    "syndata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use RandomForestClassifier model to predict success of terrorist events based on synthetic dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use our classification Model to predict successful attacks of the generated dataset\n",
    "predicted = model.predict(syndata)\n",
    "\n",
    "results= pd.DataFrame(predicted,columns = [\"Predicted_Terrorism\"])\n",
    "\n",
    "summary = pd.concat([syndata,results],axis=1)\n",
    "summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, `Predicted_Terrorism` refers to whether or not a terrorist attack is successful \n",
    "\n",
    "1 --> **successful**\n",
    "\n",
    "0 --> **unsuccessful**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_uWMmJlM3CTN"
   },
   "outputs": [],
   "source": [
    "summary.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-EUlNA2E3DPd"
   },
   "outputs": [],
   "source": [
    "y.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Country \\n\",cntry)\n",
    "print(\"\\n Weapon type \\n\",weapx)\n",
    "print(\"\\n Attack type \\n\",atk)\n",
    "print(\"\\n Target type \\n\",targ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "newdf = summary.replace({\"country\":cntry})\n",
    "newdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Information Presentation\n",
    "How can we better visualise our prediction findings?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Setting up the world countries data URL\n",
    "url = 'https://raw.githubusercontent.com/python-visualization/folium/master/examples/data'\n",
    "country_shapes = f'{url}/world-countries.json'\n",
    "#Getting the amount of successful terrorism count per country\n",
    "\n",
    "#Drop cells with predicted_terrorism 0\n",
    "clean=newdf.drop(newdf[newdf.Predicted_Terrorism ==0].index)\n",
    "a=clean[\"country\"].value_counts()\n",
    "adf= pd.DataFrame(a)\n",
    "#Reset so the index will now be a column\n",
    "adf.reset_index(inplace=True)\n",
    "#Now rename the columns\n",
    "adf=adf.rename(columns={'index':'country','country':'count'})\n",
    "print(adf.head(n=10).to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactive Chloropleth map showing distribution of predicted attacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "m=folium.Map()\n",
    "folium.Choropleth(\n",
    "    #The GeoJSON data to represent the world country\n",
    "    geo_data=country_shapes,\n",
    "    name='Predicted Terrorism events in 2022',\n",
    "    data=adf,\n",
    "    #The column accepting list with 2 value; The country name and  the numerical value\n",
    "    columns=['country','count'],\n",
    "    key_on='feature.properties.name',\n",
    "    fill_color='PuRd',\n",
    "    nan_fill_color='white'\n",
    ").add_to(m)\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2ASuHW6xqNbv"
   },
   "source": [
    "## Statistical Inference\n",
    "\n",
    " ---\n",
    "Based on the above Chloropleth map, we note that:\n",
    "1) Countries such as `Iraq` are under --> **High Risk** of successfully being attacked\n",
    "\n",
    "2) ``Afghanistan``, ``Pakistan`` are under --> **Moderate to High Risk**\n",
    "\n",
    "3) ``Colombia``, ``Peru``, `Salvador`, ``Nigeria``, ``Somalia``, ``Yemen``, ``Turkey``, ``India``, ``Thailand``, ``Philippines`` are under --> **Moderate Risk**\n",
    "\n",
    "4) Others --> **Relatively Low Risk**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pjaut6RkqRFP"
   },
   "source": [
    "## Ethical Consideration\n",
    "\n",
    " ---\n",
    "While our model can be of great use to national security teams of the above countries as they prepare to monitor future threats, we must also take into consideration that such information can easily fall into the wrong hands. If future terrorists get their hands on such prediction insights it will make it easier for them to 'escape the radar', which leads to more damage in the long run.\n",
    "\n",
    "Governments may benefit from using this prediction model as part of their arsenal of anti-terrorism measures, but never in place of it - nothing can replace the human intuition of deciding whether to heed or ignore a terrorism warning report."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6ozTMcmtqXJf"
   },
   "source": [
    "## Intelligent Decision\n",
    "\n",
    " ---\n",
    "**Recommendations:** Having taken care of the ethical considerations, our data-driven insights are as follows:\n",
    "\n",
    "**1)** With the map as a reference, countries that not only fall within the moderate- to high-risk categories, but also countries that share borders with these nations (e.g. Iran, Syria, Oman) need to ramp up their monitoring regimes. Although this prediction is not definitive and fails to account for 2020's drop in terrorist activity, it can give good insight as to what terrorists' next move(s) could be. \n",
    "\n",
    "**2)** Instead of standalone terrorism success-predictor, this intelligence advisor can be used to supplement existing intelligence strategies for security teams. They can use them as tools to delegate the available resources for surveillance and defence as and when needed. Prevention is always better - governments can additionally choose to monitor the levels of terrorist recruitment taking place in their respective countries. This data can further add value to future anti-terrorist campaigns.\n",
    "\n",
    "**3)** With the advent of new technologies and the Covid-19 Pandemic, the world has seen a rise in terrorist activity as a direct result of the Internet and online resources. Therefore, a prediction model that predicts future activities by region based on a population's online activity (E.g. by monitoring Google search history) could be a possibility although question marks over privacy exist. Nevertheless it is important to develop different dimensions in order to protect oneself on all fronts from potential terrorism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "SC1015_DS_Mini_project",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
